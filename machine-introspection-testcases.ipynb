{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A timbr-machine works by ingesting data from a queue and processing it in a nested group of user-defined functions via a dask infrastructure in a thread. Optionally, a data source that takes a generator can be run which calls .next() and puts the result in the queue, and runs in a thread. \n",
    "\n",
    "A user can run a machine in an ipython kernel. Combined with juno-magic, a presumably frequest use case might involve running a timbr-machine in an external ipython kernel somewhere as a long-running pipeline. In general, it's useful to have some amount of introspection into both the active state of the machine as well as general descriptive overall metrics. We'll look at some potential introspective methods and the architecture behind them.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twython import TwythonStreamer\n",
    "import collections\n",
    "from threading import Thread\n",
    "\n",
    "from __future__ import print_function\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timbr-machine imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import dask as da\n",
    "# NOTE: sync mode wil likely be faster\n",
    "from dask.async import get_sync as get\n",
    "# from dask.threaded import get\n",
    "\n",
    "_pool = ThreadPool()\n",
    "da.set_options(pool=_pool)\n",
    "\n",
    "try:\n",
    "    from Queue import Empty, Full, Queue # Python 2\n",
    "except ImportError:\n",
    "    from queue import Empty, Full, Queue # Python 3\n",
    "\n",
    "from bson.objectid import ObjectId\n",
    "from functools import wraps # should be used but isn't currently\n",
    "import inspect\n",
    "\n",
    "import zmq\n",
    "import json\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StoppableThread(threading.Thread):\n",
    "    \"\"\"Thread class with a stop() method. The thread itself has to check\n",
    "    regularly for the stopped() condition.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StoppableThread, self).__init__()\n",
    "        self._stop = threading.Event()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop.set()\n",
    "\n",
    "    def stopped(self):\n",
    "        return self._stop.isSet()\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def wrap_transform(fn):\n",
    "    \"\"\"\n",
    "    This function returns a new function that accepts an arbitrary number of arguments\n",
    "    and calls the wrapped function with the number of arguments that it supports. For\n",
    "    example:\n",
    "\n",
    "    def f(a, b):\n",
    "        ...\n",
    "\n",
    "    g = wrap_transform(f)\n",
    "\n",
    "    assert g(a, b, c, d) == f(a, b)\n",
    "\n",
    "    \"\"\"\n",
    "    assert callable(fn)\n",
    "    try:\n",
    "        info = inspect.getargspec(fn)\n",
    "        nargs = len(info.args)\n",
    "    except TypeError:\n",
    "        # fallback to pipeline mode\n",
    "        nargs = 1\n",
    "    def wrapped(*args, **kwargs):\n",
    "        # print(\"called with {}\".format(str(args)))\n",
    "        return fn(*args[:nargs])\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def json_serializable_exception(e, extra_data={}):\n",
    "    #extra_data[\"_traceback\"] = tb.format_tb(e)\n",
    "    #extra_data[\"_exception\"] = tb.format_exception_only(e)\n",
    "    extra_data[\"_exception\"] = str(e)\n",
    "    #extra_data[\"_exception_dict\"] = e.__dict__\n",
    "    return(extra_data)\n",
    "\n",
    "import os, errno\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else: raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaseMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_serialize(obj):\n",
    "    try:\n",
    "        return json.dumps(obj)\n",
    "    except TypeError as te:\n",
    "        return json.dumps(json_serializable_exception(te))\n",
    "\n",
    "\n",
    "class BaseMachine(object):\n",
    "    def __init__(self, stages=8, bufsize=1024):\n",
    "        self.q = Queue(bufsize)\n",
    "        self.tbl = {}\n",
    "        self.stages = stages\n",
    "        self._dsk = None\n",
    "        self._dirty = True\n",
    "        self._getter = get\n",
    "        self._socket = None\n",
    "\n",
    "\n",
    "        self.serialize_fn = json_serialize\n",
    "\n",
    "        self.REFERENCE_DASK = {\n",
    "            \"oid_s\": (str, \"oid\"),\n",
    "            \"in_s\": (self.serialize_fn, \"in\")\n",
    "        }\n",
    "        self.REFERENCE_DASK.update({\"f{}_s\".format(i): (self.serialize_fn, \"f{}\".format(i)) for i in xrange(self.stages)})\n",
    "\n",
    "    def put(self, msg):\n",
    "        # NOTE: Non-blocking\n",
    "        self.q.put(msg, False)\n",
    "\n",
    "    def get(self, block=False, timeout=0.5):\n",
    "        dsk = dict(self.dsk)\n",
    "        dsk[\"in\"] = (self.q.get, block, timeout)\n",
    "        output = self._getter(dsk, [\"oid_s\", \"in_s\"] + [\"f{}_s\".format(i) for i in xrange(self.stages)])\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return stages\n",
    "\n",
    "    def __setitem__(self, pos, fn):\n",
    "        assert isinstance(pos, (int, long))\n",
    "        assert pos >=0 and pos < self.stages\n",
    "        wrapped_fn = wrap_transform(fn)\n",
    "        self.tbl[\"f{}\".format(pos)] = wrapped_fn\n",
    "        self.dirty = True\n",
    "\n",
    "    def __getitem__(self, pos):\n",
    "        assert isinstance(pos, (int, long))\n",
    "        assert pos >=0 and pos < self.stages\n",
    "        return self.tbl[\"f{}\".format(pos)]\n",
    "\n",
    "    def __missing__(self, pos):\n",
    "        return wrap_transform(identity)\n",
    "\n",
    "    def __delitem__(self, pos):\n",
    "        assert isinstance(pos, (int, long))\n",
    "        assert pos >=0 and pos < self.stages\n",
    "        del self.tbl[\"f{}\".format(pos)]\n",
    "        self.dirty = True\n",
    "\n",
    "    def __call__(self, data, include_serialized=False):\n",
    "        dsk = dict(self.dsk)\n",
    "        dsk[\"in\"] = data\n",
    "        args = [\"oid\", \"in\"] + [\"f{}\".format(i) for i in xrange(self.stages)]\n",
    "        if include_serialized:\n",
    "            args.extend([\"oid_s\", \"in_s\"] + [\"f{}_s\".format(i) for i in xrange(self.stages)])\n",
    "\n",
    "        output = self._getter(dsk, args)\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def dsk(self):\n",
    "        if self._dsk is None or self.dirty:\n",
    "            self._dsk = {}\n",
    "            self._dsk[\"oid\"] = (ObjectId,)\n",
    "            for i in xrange(self.stages):\n",
    "                fkey = \"f{}\".format(i)\n",
    "                cmd = [self.tbl.get(fkey, wrap_transform(identity))]\n",
    "                cmd.extend([\"f{}\".format(j) for j in reversed(xrange(i)) if i > 0])\n",
    "                cmd.append(\"in\")\n",
    "                cmd.extend([\"f{}_s\".format(j) for j in reversed(xrange(i)) if i > 0])\n",
    "                cmd.append(\"in_s\")\n",
    "                self._dsk[fkey] = tuple(cmd)\n",
    "            self._dsk.update(self.REFERENCE_DASK)\n",
    "            self.dirty = False\n",
    "        return self._dsk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MachineConsumer(StoppableThread):\n",
    "    def __init__(self, machine):\n",
    "        super(MachineConsumer, self).__init__()\n",
    "        self.machine = machine\n",
    "        self._socket = None\n",
    "        # set local kernel key\n",
    "        with open(get_ipython().config[\"IPKernelApp\"][\"connection_file\"]) as f:\n",
    "            config = json.load(f)\n",
    "            self._kernel_key = config[\"key\"]\n",
    "        mkdir_p(\"/tmp/timbr-machine\") # NOTE: Not Windows Safe (but should be)\n",
    "        self.initialize_pub_stream(\"ipc:///tmp/timbr-machine/{}\".format(self._kernel_key))\n",
    "\n",
    "    def initialize_pub_stream(self, endpoint):\n",
    "        ctx = zmq.Context()\n",
    "        self._socket = ctx.socket(zmq.PUB)\n",
    "        self._socket.bind(endpoint)\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stopped():\n",
    "            try:\n",
    "                # NOTE: self.get should never throw exceptions from inside the dask\n",
    "                output = self.machine.get(block=True, timeout=0.1)\n",
    "                hdr = output[0]\n",
    "                msg = \"[{}]\".format(\",\".join(output[1:]))\n",
    "                # print(output)\n",
    "                self._socket.send_multipart([hdr, msg.encode(\"utf-8\")])\n",
    "            except Empty:\n",
    "                pass\n",
    "\n",
    "\n",
    "class SourceConsumer(StoppableThread):\n",
    "    def __init__(self, machine, generator):\n",
    "        super(SourceConsumer, self).__init__()\n",
    "        self.g = generator\n",
    "        self.machine = machine\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stopped():\n",
    "            try:\n",
    "                # NOTE: next() may block which is okay but put may raise Full\n",
    "                # which will interrupt the source\n",
    "                msg = self.g.next()\n",
    "                self.machine.put(msg)\n",
    "            except (StopIteration, Full):\n",
    "                break\n",
    "\n",
    "\n",
    "class Machine(BaseMachine):\n",
    "    def __init__(self, stages=8, bufsize=1024):\n",
    "        super(Machine, self).__init__(stages, bufsize)\n",
    "        self._consumer_thread = None\n",
    "        self._running = False\n",
    "\n",
    "    def start(self):\n",
    "        if not self._running:\n",
    "            self._consumer_thread = MachineConsumer(self)\n",
    "            self._consumer_thread.start()\n",
    "            self._running = True\n",
    "\n",
    "    def stop(self):\n",
    "        self._consumer_thread.stop()\n",
    "        time.sleep(0.2) # give the thread a chance to stop\n",
    "        self._running = False\n",
    "\n",
    "    def set_source(self, source_generator):\n",
    "        self._source = SourceConsumer(self, source_generator)\n",
    "        self._source.start()\n",
    "        return self._source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MACHINE = Machine()\n",
    "\n",
    "def raise_error_every_now_and_then(x):\n",
    "    if recent_data._count > 1 and recent_data._count % 31 == 0:\n",
    "        raise TypeError\n",
    "\n",
    "MACHINE[0] = lambda x: x.get(\"text\", \"\")\n",
    "MACHINE[1] = lambda x: TextBlob(x).sentiment\n",
    "MACHINE[2] = lambda a, b: recent_data.append((b, a))\n",
    "MACHINE[3] = raise_error_every_now_and_then\n",
    "\n",
    "class CountingDeque(collections.deque):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._count = 0\n",
    "        super(CountingDeque, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    def append(self, *args):\n",
    "        super(CountingDeque, self).append(*args)\n",
    "        self._count += 1        \n",
    "        \n",
    "recent_data = CountingDeque(maxlen=50)\n",
    "#thatsdogs\n",
    "app_key = \"is4Leas6P8ajv4ERNojyJ7psg\"\n",
    "app_secret = \"JIb2EEGWbE6NmS4NbdMCARfoCONdYxwG6mfnLY9Z61Q9ZkM9cD\"\n",
    "access_token = \"1881035263-ghn9BPqkY4PMyVdfsuaNEeTYBtRXwfKo8Op07Cw\"\n",
    "token_secret = \"uKrTloEChEQWShUfU3FS9ejXyi906HjHbsB4T4QDtE7HW\"\n",
    "\n",
    "class Streamer(TwythonStreamer):\n",
    "    \n",
    "    def on_success(self, data):\n",
    "        #print data\n",
    "        MACHINE.put(data)\n",
    "        \n",
    "#     def on_error(self, status_code, data):\n",
    "#         MACHINE.put({\"error\": status_code})\n",
    "\n",
    "streamer = Streamer(app_key, app_secret, access_token, token_secret)\n",
    "streamthread = Thread(target= streamer.statuses.filter, kwargs={\"track\": \"trump\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test case puts tweets onto the machine queue at a high rate. We've included a function that throws an arbitrary error every 31 tweets so that we can investitage pipeline error behavior and various ways to deal with errors in our pipeline. \n",
    "\n",
    "First, we'll see how the machine behaves when an error is raised inside the dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-a506283690e7>\", line 22, in run\n",
      "    output = self.machine.get(block=True, timeout=0.1)\n",
      "  File \"<ipython-input-4-c6caacfa5d44>\", line 34, in get\n",
      "    output = self._getter(dsk, [\"oid_s\", \"in_s\"] + [\"f{}_s\".format(i) for i in xrange(self.stages)])\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 519, in get_sync\n",
      "    raise_on_exception=True, **kwargs)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 490, in get_async\n",
      "    fire_task()\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 461, in fire_task\n",
      "    get_id, raise_on_exception])\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 511, in apply_sync\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 267, in execute_task\n",
      "    result = _execute_task(task, data)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 249, in _execute_task\n",
      "    return func(*args2)\n",
      "  File \"<ipython-input-2-1dd02d40106b>\", line 43, in wrapped\n",
      "    return fn(*args[:nargs])\n",
      "  File \"<ipython-input-21-831e1ec9c551>\", line 5, in raise_error_every_now_and_then\n",
      "    raise TypeError\n",
      "TypeError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MACHINE.start()\n",
    "streamthread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/twython/streaming/types.py\", line 66, in filter\n",
      "    self.streamer._request(url, 'POST', params=params)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/twython/streaming/api.py\", line 154, in _request\n",
      "    if self.on_success(data):  # pragma: no cover\n",
      "  File \"<ipython-input-21-831e1ec9c551>\", line 32, in on_success\n",
      "    MACHINE.put(data)\n",
      "  File \"<ipython-input-4-c6caacfa5d44>\", line 29, in put\n",
      "    self.q.put(msg, False)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/Queue.py\", line 123, in put\n",
      "    raise Full\n",
      "Full\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recent_data._count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    streamer.disconnect()\n",
    "    MACHINE.stop()\n",
    "    del streamer\n",
    "    del MACHINE\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, once the machine errored out, it stopped running, and consequently stopped taking items off of its queue, which eventually raised Full. Let's see what's going on with the consumer thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(MACHINE._running)\n",
    "print(MACHINE._consumer_thread.stopped())\n",
    "print(MACHINE._consumer_thread.isAlive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thread running our machine consumer has shutdown after the error was raised, but our machine hasn't registered that information because our StoppableThread baseclass terminated on an unhandled exception.\n",
    "\n",
    "Notably, our only method of checking that our MACHINE was running (and that it had effectively stopped) was by appending results to a custom counting deque.\n",
    "\n",
    "It's immediately apparent that we lack insight into the \"state\" of the machine as it is operating \"now\", not to mention how it has operated in the past (except maybe that no errors occured until that one). Assuming we actively seek the path to enlightenment, we're first tasked to define machine state as it is most relevent to our purposes.\n",
    "\n",
    "We can start by asking the same, very relevant question we first asked in our experimental field work: is this running or not? That question is important, because we're presumably interfacing with our machine to accomplish some task, and the answer determines our next action in real life as an end-user, even if that's to do nothing. So, in the event we don't have access to that information, we will wait forever and die, or else try to glean clues via outside artifacts like above or some other way that's shitty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Machine(BaseMachine):\n",
    "    def __init__(self, stages=8, bufsize=1024):\n",
    "        super(Machine, self).__init__(stages, bufsize)\n",
    "        self._consumer_thread = None\n",
    "#-         self._running = False\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self._consumer_thread = MachineConsumer(self)\n",
    "            self._consumer_thread.start()\n",
    "#-             self._running = True\n",
    "\n",
    "    def stop(self):\n",
    "        self._consumer_thread.stop()\n",
    "        time.sleep(0.2) # give the thread a chance to stop\n",
    "#-         self._running = False\n",
    "\n",
    "    def set_source(self, source_generator):\n",
    "        self._source = SourceConsumer(self, source_generator)\n",
    "        self._source.start()\n",
    "        return self._source\n",
    "#++++    \n",
    "    @property\n",
    "    def running(self):\n",
    "        if self._consumer_thread is None:\n",
    "            return False\n",
    "        return self._consumer_thread.is_alive()\n",
    "#++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-a506283690e7>\", line 22, in run\n",
      "    output = self.machine.get(block=True, timeout=0.1)\n",
      "  File \"<ipython-input-4-c6caacfa5d44>\", line 34, in get\n",
      "    output = self._getter(dsk, [\"oid_s\", \"in_s\"] + [\"f{}_s\".format(i) for i in xrange(self.stages)])\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 519, in get_sync\n",
      "    raise_on_exception=True, **kwargs)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 490, in get_async\n",
      "    fire_task()\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 461, in fire_task\n",
      "    get_id, raise_on_exception])\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 511, in apply_sync\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 267, in execute_task\n",
      "    result = _execute_task(task, data)\n",
      "  File \"/Users/jamiepolackwich1/anaconda/envs/juno-machine/lib/python2.7/site-packages/dask-0.10.0-py2.7.egg/dask/async.py\", line 249, in _execute_task\n",
      "    return func(*args2)\n",
      "  File \"<ipython-input-2-1dd02d40106b>\", line 43, in wrapped\n",
      "    return fn(*args[:nargs])\n",
      "  File \"<ipython-input-104-831e1ec9c551>\", line 5, in raise_error_every_now_and_then\n",
      "    raise TypeError\n",
      "TypeError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MACHINE.start()\n",
    "streamthread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MACHINE.running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatever a machine state is, it's obviously something that's built to change in time, because we're using it like a data processing pipeline. So figuring out a consistent way to talk about the things that change is a good thing probs.\n",
    "\n",
    "It's useful to consider the idea of an event. An event is something that happens at some time that can change the state of the machine. Using our test-case as an example, a user-defined dask function throwing an error shutoff our machine, which qualifies. Putting data on a full machine queue can also break our machine, however that type of event is inherently trigger by some other event, like a processing bottleneck or a broken consumer thread. The idea is to have enough information at any time avilable to us so that we can whatever next steps we'd like efficiently.  \n",
    "\n",
    "Although that line of thinking implies some approach to handling errors from different events, it's also useful for dealing with other kinds of events. For instance, the absence of data in the queue after some time might signify that some source is exhausted. In such a case it might be prudent to stop the machine and issue a notification of completion. Progress can be measured via data processed; in the event that errors are being handled, where and how often might move a user to go back and change things; in general, if we design our event handling system to do things, we still ought to be able to discern that events are being handled from our information system.\n",
    "\n",
    "Despite the use case, the same core machine will serve every pipeline. We can define core metrics that describe our machine state consistently in any case. If they're really fundamental state metrics, we can define them in BaseMachine along with a base_display maybe (without actually subclassing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from IPython.display import Image, HTML, display\n",
    "\n",
    "\n",
    "class BaseMachineWithStats(BaseMachine):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(BaseMachineWithStats, self).__init__(*args, **kwargs)\n",
    "        self._status = {\"CurrentOID\": None, \"LastOID\": None, \"Processed\": 0, \"QueueSize\": self.q.qsize()}\n",
    "    \n",
    "    def get(self, *args, **kwargs):\n",
    "        self._before_get(*args, **kwargs)\n",
    "        result = super(BaseMachine, self).get(*args, **kwargs)\n",
    "        self._after_get(result)\n",
    "        \n",
    "    @property\n",
    "    def status(self):\n",
    "        return self._status\n",
    "    \n",
    "    #default\n",
    "    def display_status(self):\n",
    "        stats = self.status\n",
    "        s0 = \"<div style='border:1px; border-style:solid; width:400px; height:auto; float:left;'><b>Current Ingest Time -- {}</b></div>\".format(stats['CurrentTime'])\n",
    "        s1 = \"<div style='border:1px; border-style:solid; width:400px; height:auto; float:left;'><b>Current Ingest ID -- {}</b></div>\".format(stats['CurrentOID'])\n",
    "        s2 = \"<div style='border:1px; border-style:solid; width:400px; height:auto; float:left;'><b>Total Datum Processed -- {}</b></div>\".format(stats['Processed'])\n",
    "        s3 = \"<div style='border:1px; border-style:solid; width:400px; height:auto; float:left;'><b>Current Queue Depth -- {}</b></div>\".format(stats[\"QueueSize\"])\n",
    "        display(HTML(\"\\n\".join([s0, s1, s2, s3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it could also be interesting to be able to configure custom status hook plugins. In this way, we could write templates for situations where a fixed number of data is known beforehand, for instance, and display_status might include a progress bar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach could be used to create event handler templates for specific scenarious. For instance, a \"RawEventDebugger\" event handler template might raise all possible errors locally, shutdown the machine, and provide in-depth tracebacks or something. \"DaskEventDebugger\" might only raise dask errors, etc. After learning about the pipeline you're developing, you can use a \"BaseEventHandler\" or customize your own. Shutdown and Restart hooks could be implemented.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statushook(method):\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(inst, *args, **kwargs):\n",
    "        try:\n",
    "            inst.__getattribute__('_pre_' + method.im_func.func_name)(*args, **kwargs)\n",
    "        except AttributeError as ae:\n",
    "            pass\n",
    "        result = method(*args, **kwargs)\n",
    "        kwargs.update('result', result)\n",
    "        try:\n",
    "            inst.__getattribute__('_post_' + method.im_func.func_name)(*args, **kwargs)\n",
    "        except AttributeError as ae:\n",
    "            pass\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General class for making tables.  Pass in header, iterable/s of iterable/s\n",
    "class StatusTable(object):    \n",
    "    def __init__(self, status, headers=None, title=None, cell_padding=False, abrv_links = False):\n",
    "        self.nitr = iterable_of_iterables\n",
    "        self.headers = headers\n",
    "        self.title = title\n",
    "        self.htmls = ''\n",
    "        self.abrv_links = abrv_links\n",
    "        self.cell_padding = cell_padding\n",
    "    \n",
    "    def html_tag(self, obj):\n",
    "        \n",
    "        #obj = str(obj)\n",
    "        if 'http' in obj:\n",
    "            if self.abrv_links:\n",
    "                return '<td><a href = ' + obj.encode('utf-8') + '>' + '...' + obj[-10:].encode('utf-8') + '</a></td>'\n",
    "            else:\n",
    "                return '<td><a href = ' + obj.encode('utf-8') + '>' + obj.encode('utf-8') + '</a></td>'\n",
    "        else:\n",
    "            return '<td>' + obj.encode('utf-8') + '</td>'\n",
    "\n",
    "    def table_view(self):\n",
    "        if self.title != None:\n",
    "            self.htmls += '<h3 align=center>{}</h3>'.format(self.title)\n",
    "        if self.cell_padding:\n",
    "            self.htmls += '<table border=1 cellpadding=3; style=width:100%>\\n'\n",
    "        else:\n",
    "            self.htmls += '<table border=1; style=width:100%>\\n'\n",
    "        if self.headers != None:\n",
    "            self.htmls += '<tr>' + ''.join([self.html_tag('<b>' + _h + '</b>') for _h in self.headers])\n",
    "        for itr in self.nitr:\n",
    "            self.htmls += '<tr>' + ''.join([self.html_tag(_obj) for _obj in itr]) + '</tr>\\n'\n",
    "        self.htmls += '</table>'\n",
    "        display(HTML(self.htmls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
